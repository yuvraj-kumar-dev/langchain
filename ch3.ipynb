{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656587f2",
   "metadata": {},
   "source": [
    "# Deploy LLM as API\n",
    "\n",
    "So, now you can easily make chatbots or conversational bots using both open source OLLAMA LLMs and OPENAI LLMs, so we made these chatbots using chains where we have to first make a prompt template, then load llm and then get the output as a string but in real world scenarios we would like to easily integrate these chatbots in different platforms like apps and websites rather than creating chains for each of them separately, which can be easily done using **APIs** In this tutorial we will learn how can we makes APIs of these chains using the amazing langchain ecosystem where we are provided **Langserve** which will help us to create APIs using **FastAPI** which will further allow us to easily deployment purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934437d0",
   "metadata": {},
   "source": [
    "## Langserve : It helps developers deploy Langchain runnables and chains as REST API. This library isi integrated with FASTAPI and uses PYNDATIC for data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98cbed",
   "metadata": {},
   "source": [
    "### Required Libraries : requirements.txt\n",
    "\n",
    "> 1) langchain_openai\n",
    "> 2) langchain_core\n",
    "> 3) python-dotenv\n",
    "> 4) streamlit\n",
    "> 5) langchain_community\n",
    "> 6) langserve\n",
    "> 7) fastapi\n",
    "> 8) uvicorn\n",
    "> 9) sse_starlette\n",
    "\n",
    "install all these libraries in your virtual environment using one of the techniquies discussed in **Chapter:2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae81c9b",
   "metadata": {},
   "source": [
    "### app.py\n",
    "\n",
    "```python\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from fastapi import FastAPI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Langchain Server\",\n",
    "    description=\"A simple API server\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    ChatOpenAI(),\n",
    "    path='/openai'\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "#Ollama llm\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "#chat prompt template\n",
    "prompt1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"Give me 5 synonyms of {word}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"Give me best five uses of {word}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt1 | model,\n",
    "    path = \"/synonyms\"\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt2 | llm,\n",
    "    path = \"/uses\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068e98b",
   "metadata": {},
   "source": [
    "after setting up app.py run app.py by `python app.py`\n",
    "\n",
    "Now go to localhost and check if server is running and then go to `http://localhost:8000/docs` you will see all the defined routes in your langchain server\n",
    "\n",
    "Now our next step is how we can easily use or access these apis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afa7b9",
   "metadata": {},
   "source": [
    "### client.py \n",
    "\n",
    "Now we will learn how can we use the api created by us in different platforms like websites so we are going to create a simple website using streamlit and use the above api created to generate some content in this website using the api routes /synonyms and /uses | Here we will create a streamlit application and call these APIs\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "def openai_response(input_text):\n",
    "    response = requests.post(\"http://localhost:8000/synonyms/invoke\", \n",
    "                        json={'input':{'word': input_text }})\n",
    "    return response.josn()['output']['content']\n",
    "\n",
    "def ollama_response(input_text):\n",
    "    response = requests.post(\"http://localhost:8000/uses/invoke\", \n",
    "                        json={'input':{'word': input_text }})\n",
    "    return response.josn()['output']\n",
    "                            \n",
    "\n",
    "\n",
    "input_text1 = st.input_text(\"Write the word whose synonym you want to know\")\n",
    "input_text2 = st.input_text(\"Write the word whose uses you want to know\")\n",
    "\n",
    "if input_text1:\n",
    "    response = openai_response(input_text1)\n",
    "    st.write(response)\n",
    "\n",
    "if input_text2:\n",
    "    response = ollama_response(input_text2)\n",
    "    st.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0f860",
   "metadata": {},
   "source": [
    "Now in the above code the client is interacting with the API"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
